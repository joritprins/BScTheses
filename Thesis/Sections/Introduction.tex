\documentclass[../thesis.tex]{subfiles}
% \graphicspath{{../Images/}} % <- working!
% \graphicspath{{\subfix{../Images/}}}

\begin{document}
\section{Relevance}
\begin{figure}
    \centering
    \subfile{../Graphs/MLaaS}   
    \caption{Inference process in Machine Learning as a Service (MLaaS). The NN is already trained by the server and the client wants to know the output of the NN: $f(x)$}
    \label{fig:mlaas}
\end{figure}
The recent rise in Big Data increased the data exchange on the internet. With more and more computer resources available, researchers quickly started to utilise the possibilities of machine learning (ML) to analyse the data.  Techniques like neural networks (NN) are promising ways to scientific breakthroughs. Machine learning has a wide variety of applications for classification such as traffic analysis, image recognition, intrusion detection,  spam detection, medical or genomics predictions,  financial predictions and face recognition \parencite{dowlin2017,islam2011, bachrach16, kaiming215}

The use of ML typically consists of two phases: training and inference. In the first phase a NN is trained by feeding an extensive dataset to find the best parameters. NNs used for machine learning have to be maintained, evaluated and the training phase is often a tedious and time exhausting process. In the inference phase an input is applied to the trained NN. Because of the time consuming process of creating a NN, machine learning as a service (MLaaS) became popular \parencite{ribeiro2015mlaasml}. In MLaaS, a company offers a pre-trained NN to the clients. Now, clients only need to worry about the inference phase.


A typical MLaaS situation (figure \ref{fig:mlaas}) consist of two parties: the client holding an input \textit{x} and a company holding neural network \textit{f}. This research focuses on the inference part: the client wants to know the neural network (held by the server) applied to input \textit{x} (held by the client): \textit{f(x)}.

% However, MLaaS offers great threats to privacy, , while keeping the (sensitive) contents of \textit{x} and the result \textit{f(x)} private from the company. The company wants to hold the intellectual property \textit{f} private while still giving the opportunity to the client to use \textit{f} to obtain \textit{f(x)}. 

However, MLaaS offers great threats to privacy. For the server to train the model as accurately as possible a NN needs access to a large amount of precise data, which may consist of sensitive information. Data providers may be reluctant to provide the server with this data because of the sensitive properties of the data. Besides, other features, irrelevant to the prediction task, could also be derived from this data \parencite{nasr2019}. Moreover, input from the client can also be confidential and the client can therefore be reluctant to send $x$ to the server. On the other hand, owners of a NN could be worried that an adversary could steal (parameters of) their (often costly) NN. Furthermore, the result ($f(x)$) of the NN could also be confidential resulting in the need to retain this information from unauthorized parties. The secure neural network inference (SNNI) problem entails calculating the applied input \textit{f(x)} while still holding all the above security requirements. 

% Thus, clients may be reluctant to provide the NNs with their data. Other features, irrelevant to the prediction task, could also be derived from this data \parencite{Nasr2019}. On the inference phase, input from the client to the NN can also be confidential. On the other hand, owners of a NN could be worried that an adversary could steal (parameters of) their (often costly) NN. Furthermore, the result of the NN could also be confidential resulting in the need to retain this information from unauthorized parties. The secure neural network inference (SNNI) problem entails calculating the applied input \textit{f(x)} while still holding all the above security requirements. 

No general implementation of an SNNI has been widely accepted. Rapid progress in this area has made it hard to get a good overview of technological advances. Mann et al. (\citeyear{mann22}) has summarized several proposed approaches for SNNI. However, these approaches are often proof-of-concept and are not thoroughly tested. Moreover, the performance is often only tested on basic measures like efficiency or accuracy. Other metrics like energy consumption, that could be of relevance, are not researched. This could be of importance because of limitations on the client side. For example when a device is battery powered. One other example is in the case of IoT devices that have limited power resources following that the overall energy consumption should be low. Companies, on the other hand, also want to keep energy consumption as low as possible because of budget limitations and thus should not encounter big energy overhead. Another reason to limit the energy consumption is the desire to reduce carbon emission in the fight against climate change.

\section{Research question(s)}
To contribute to the prior research in this area, I will discuss the energy implications of a suggested, open source implementation of an approach to SNNI. A few of these implementations are ABY2.0 \parencite{aby20}, Chameleon \parencite{chameleon}, Cheetah \parencite{cheetah}, CrypTFlow2 \parencite{cryptflow2} and Delphi \parencite{delphi}. 


\begin{table}[]
    \begin{adjustbox}{width=\columnwidth,center}
        \subfile{../Tables/Cheetah_table_7}
        \quad
        \subfile{../Tables/Cheetah_table_8}
    \end{adjustbox}
    \caption{Table 7 (left) and 8 (right) from \parencite[p. 821]{cheetah}: running time and communication costs of Cheetah compared to Delphi and CryptFlow2 (\textit{${SCI}_{HE}$})}
    \label{table:cheetah_table78}
\end{table}

For this research I will test the approach called Cheetah \parencite{cheetah}. This has several reasons. Firstly it is one of the most recent papers in this area (research also included in the 2022 Proceedings of the 31st USENIX Security Symposium). Second, they provided a working proof-of-concept on Github\footnote{Their implementation is available from https://github.com/
Alibaba-Gemini-Lab/OpenCheetah. I will use the commit \href{https://github.com/Alibaba-Gemini-Lab/OpenCheetah/commit/09fe195d26744e0d39a4d1fbfac66f31241a0a67}{09fe195} for this research.} and the last commit on GitHub was in July 2022. Third, the proof-of-concept implementation has been tested on a WAN and LAN network and on two different devices. This is good, because it represents the most realistic MLaaS scenario \parencite{ribeiro2015mlaasml} and it is not necessary anymore to adapt the code. Fourth, it claims to achieve better speedup than the other recent approaches (e.g. Delphi and CryptFlow2, see Table ~\ref{table:cheetah_table78}). Last, one author \parencite{dong2022} has already used aforementioned approaches as building blocks and also saw significant performance improvements when using Cheetah over others.



The main research question is of this project is:

\begin{quote} \emph{RQ: What are the energy implications of Cheetah compared to its counterpart CryptFlow2?} \end{quote} 

\noindent To help research the implications of the SNNI, I have defined a subset of research questions:

\begin{quote} \emph{RQa: How do we best measure the energy consumption of a SNNI?} \end{quote}

\noindent Once I have established a way to measure energy consumption of SNNIs, I can start with the experiments of measuring the energy consumption. I will be calculating the overhead of Cheetah. With these results, the difference between the overhead on the client side and the overhead on the server side can be calculated. If there is a difference I shortly want to look at the implications of this difference. This implications could for example have impact on the aforementioned IoT devices or carbon emission, thus resulting in research question \textit{RQb}:

\begin{quote} \emph{RQb: If there is a difference between energy overhead on the client side and on the server side: what are the implications of the overhead on the client side and what are implications the overhead the server side?} \end{quote}

\section{Method}
To answer these aforementioned questions first I will have to select one implementation of a SNNI to start with. Once the implementation is chosen I will need to get it working on my own setup. My setup will probably be a desktop and a laptop (on running server side and the other the client side). Simultaneous to this I will answer question \textit{RQa} with a literature search on how other authors measure energy consumption. There is a possibility that other authors have already researched the energy consumption of an approach to SNNI mentioned in the preceding section, or researched the energy implications of other programs and I can use their methods if proven successful.

Once the implementation is set up and \textit{RQa} has been answered, I can start testing the energy consumption of a NN with and without the chosen SNNI. With the results the overhead can be calculated. If there is time to set up another implementation I will compare the overhead between these implementations. With the results of this experiment I can answer the first part of \textit{RQb}. The second part of \textit{RQb} can be answered with a small literature search on the implications of energy consumption. 

\end{document}
